{
  "summary": "The document outlines the Data Migration Solution Design for the Human Resources Management Information System (HRMIS) Program, a WA Health initiative to replace legacy HR, payroll, and rostering systems with a unified, modern platform. It details the program\u2019s objectives, including improved reliability, user accessibility, and enhanced workforce management across the health system. The solution leverages commercial off-the-shelf software with minimal customisation, focusing on configuration and process improvement. Key concepts include the use of the DataRemedy tool for data migration, data quality uplift, security and scrambling, validation, and a phased implementation approach led by Health Support Services in partnership with Deloitte. The document also covers technical architecture, data sourcing, loading, transformation, and governance processes.",
  "qa_pairs": [
    {
      "question": "What is the main goal of the HRMIS Program?",
      "answer": "The main goal is to replace legacy HR, payroll, and rostering systems with a unified, modern platform for WA Health."
    },
    {
      "question": "Which tool is used for data migration in the HRMIS Program?",
      "answer": "The DataRemedy tool is used for data migration."
    },
    {
      "question": "Who leads the implementation and delivery of the HRMIS Program?",
      "answer": "Health Support Services (HSS) leads the implementation and delivery in partnership with Deloitte."
    },
    {
      "question": "What approach does the HRMIS Program take regarding software customisation?",
      "answer": "The program uses commercial off-the-shelf software with minimal customisation, focusing on configuration and process improvement."
    },
    {
      "question": "What are some key concepts included in the Data Migration Solution Design?",
      "answer": "Key concepts include data quality uplift, security and scrambling, validation, and a phased implementation approach."
    },
    {
      "question": "How many stages are there in the HRMIS Program implementation?",
      "answer": "There are five stages: Mobilisation, Design, Build, Test, and Deployment."
    },
    {
      "question": "What is the expected benefit of the new HRMIS solution for WA Health staff?",
      "answer": "It will be more reliable, user-friendly, and accessible, supporting better workforce management and reducing administrative burden."
    },
    {
      "question": "What is the main objective of the HRMIS Program for WA Health?",
      "answer": "The main objective is to replace legacy HR, payroll, and rostering systems with a unified, modern platform to improve reliability, user accessibility, and workforce management."
    },
    {
      "question": "Which tool is used for data migration in the HRMIS Program?",
      "answer": "The DataRemedy tool is used for data migration."
    },
    {
      "question": "What are some key concepts included in the Data Migration Solution Design?",
      "answer": "Key concepts include data quality uplift, security and scrambling, validation, and a phased implementation approach."
    },
    {
      "question": "Who leads the implementation of the HRMIS Program?",
      "answer": "Health Support Services leads the implementation in partnership with Deloitte."
    },
    {
      "question": "What does the Data Migration Solution Architecture provide?",
      "answer": "It provides an overview of the overall architecture of the data migration solution, including the DataRemedy toolset, its environments, and its relationship to other system components."
    },
    {
      "question": "What is the approach to data migration as outlined in the guiding principles?",
      "answer": "Data migration should be automated where possible, focus on target system requirements, source data from the HRIS Data Warehouse, and use manual cleansing only for critical data."
    },
    {
      "question": "What is the purpose of the Data Migration Solution Design document?",
      "answer": "The document outlines the end-to-end HRMIS Data Migration and Reconciliation solution design, including platform architecture, data quality remediation, and validation reports."
    },
    {
      "question": "What is the main objective of the Data Migration Solution for the HRMIS Program?",
      "answer": "The main objective is to migrate data from legacy HR, payroll, and rostering systems into the new HRMIS target systems."
    },
    {
      "question": "Which tool is used for data migration in the HRMIS Program?",
      "answer": "The DataRemedy tool is used for data migration in the HRMIS Program."
    },
    {
      "question": "What technology underpins the DataRemedy solution?",
      "answer": "DataRemedy is built primarily using Talend as its underlying ETL technology."
    },
    {
      "question": "What are the four key processes involved in HRMIS Data Migration?",
      "answer": "The four key processes are Data Sourcing, Data Quality Uplift, Data Mapping & Transformation, and Data Loading."
    },
    {
      "question": "Who leads the implementation of the HRMIS Data Migration Solution?",
      "answer": "Health Support Services leads the implementation in partnership with Deloitte."
    },
    {
      "question": "What is the purpose of Talend Data Stewardship in the DataRemedy solution?",
      "answer": "Talend Data Stewardship allows Data Stewards to clean, certify, reconcile data, and delegate tasks for manual data remediation."
    },
    {
      "question": "How does the DataRemedy solution handle version control?",
      "answer": "Version control is managed using Azure DevOps to track and manage changes and further development of jobs."
    },
    {
      "question": "What is the expected number of data loads during the Deploy stage of the HRMIS solution?",
      "answer": "The total expected number of data loads during the Deploy stage is estimated to reach more than 2000 loads."
    },
    {
      "question": "What is the purpose of the DataRemedy tool in the HRMIS Data Migration Solution?",
      "answer": "DataRemedy is used for data migration, data quality uplift, security and scrambling, validation, and orchestrating the migration process."
    },
    {
      "question": "What is the function of the Job_control_config metadata table in DataRemedy?",
      "answer": "The Job_control_config table houses business rules, links relevant jobs for identifying data quality issues, and contains remediation controls for those issues."
    },
    {
      "question": "What does the DQ_issue_register metadata table track?",
      "answer": "The DQ_issue_register table logs identified data quality issues and tracks their progress through the remediation process, including the remediated value."
    },
    {
      "question": "How does the WA Health Azure Cloud interact with Talend Cloud in the DataRemedy architecture?",
      "answer": "The WA Health Azure Cloud interacts with Talend Cloud only through metadata, ensuring no source system or HRMIS business data leaves the WA Health Cloud."
    },
    {
      "question": "What are the two main environments in DataRemedy, and why are they used?",
      "answer": "DataRemedy has TEST and PROD environments to align with HRMIS solution environments, segregate data migration activities, and enable DevOps processes."
    },
    {
      "question": "What are the key components required per environment (TEST or PROD) in DataRemedy?",
      "answer": "Each environment requires a Windows VM for compute, a Linux VM for Talend Data Stewardship, and a Windows VM with MS SQL Server for metadata and worktables."
    },
    {
      "question": "What is the primary tool used for data migration in the HRMIS Program?",
      "answer": "The primary tool used for data migration in the HRMIS Program is DataRemedy."
    },
    {
      "question": "Where are data sources in CSV format stored for access by DataRemedy?",
      "answer": "Data sources in CSV format are stored in an SFTP location for access by DataRemedy."
    },
    {
      "question": "What software is used to configure and develop DataRemedy jobs?",
      "answer": "DataRemedy jobs are configured and developed in Talend Studio."
    },
    {
      "question": "How are DataRemedy jobs scheduled and run in different environments?",
      "answer": "DataRemedy jobs are published to Talend Cloud Management Console, where they are scheduled and run on the remote engine of the respective environment (TEST or PROD)."
    },
    {
      "question": "What are the main goals of the DataRemedy solution?",
      "answer": "The main goals of the DataRemedy solution are to uplift data quality and migrate data into the target application."
    },
    {
      "question": "What is the purpose of the landing layer in DataRemedy?",
      "answer": "The landing layer stores the data retrieved from the data source in the most recent extraction run."
    },
    {
      "question": "Which layer in DataRemedy is responsible for tracking historical records and active rows?",
      "answer": "The staging layer is responsible for tracking historical records and active rows."
    },
    {
      "question": "What is the main function of the remediation layer in DataRemedy?",
      "answer": "The remediation layer is the main layer for data quality uplift and contains the full dataset with remediated values."
    },
    {
      "question": "What does the mapping layer in DataRemedy store?",
      "answer": "The mapping layer stores the tables to be loaded into the target systems, with structures defined in the templates provided by the SAP and UKG teams."
    },
    {
      "question": "How does DataRemedy handle auto remediation?",
      "answer": "DataRemedy runs identification jobs to find issues in column fields and remediates them immediately using defined remediation rules, updating the values in the remediation table."
    },
    {
      "question": "What tool is used for data migration in the HRMIS Program?",
      "answer": "The DataRemedy tool is used for data migration in the HRMIS Program."
    },
    {
      "question": "How are data quality issues identified and tracked during auto remediation?",
      "answer": "A child job identifies issues for specified table columns and stores them in the \u2018DQ_Issue_Register\u2019, which tracks the identified field, the data quality rule used, and the original value."
    },
    {
      "question": "What happens to issues that cannot be auto-remediated?",
      "answer": "Issues that cannot be auto-remediated are sent to Talend Stewardship for manual remediation by designated persons."
    },
    {
      "question": "How does the system track when a table is fully remediated?",
      "answer": "A table is tracked as fully remediated once all issues in every row are resolved and marked as complete in the remediation table."
    },
    {
      "question": "What types of data transformations are performed in the transformation layer?",
      "answer": "Transformations may include joins, value lookups, in-field value replacements, and data type or data format changes."
    },
    {
      "question": "What are the three types of data ingestion supported by DataRemedy?",
      "answer": "The three types of data ingestion are periodic, incremental, and full."
    },
    {
      "question": "Which connection types does DataRemedy handle for data ingestion?",
      "answer": "DataRemedy handles database connections, CSV extracts into blob storage, and API connections using Microsoft Graph."
    },
    {
      "question": "How is data moved from the source to the landing layer?",
      "answer": "A master job iterates through the objects in the metadata table, launching child jobs to extract data from the source and store it in the landing table after truncating the table."
    },
    {
      "question": "What is the purpose of the audit log in Talend jobs?",
      "answer": "The audit log captures stats and logs metadata about each job run, including job name, version, repository, start and end times, duration, status, and error messages."
    },
    {
      "question": "What are the three specific modules in HRMIS identified as target systems?",
      "answer": "The three specific modules are SAP SuccessFactors Employee Central (EC), SAP SuccessFactors Employee Central Payroll (ECP), and UKG Dimensions (UKG)."
    },
    {
      "question": "How is data loaded into Employee Central (EC)?",
      "answer": "Data is loaded into EC via manual imports in CSV format or automated API-based interfaces using the SuccessFactors OData API."
    },
    {
      "question": "What tool is used for manual data loading into Employee Central Payroll (ECP)?",
      "answer": "The Legacy System Migration Workbench (LSMW) tool is used for manual data loading into ECP."
    },
    {
      "question": "How does Employee Central Payroll (ECP) support automated data loading?",
      "answer": "ECP does not support direct web-based APIs out of the box, but the HRMIS solution includes the development of OData APIs to load data directly into ECP."
    },
    {
      "question": "What integration tool is used for transforming or mapping data into UKG Dimensions?",
      "answer": "The Dell Boomi tool is used for transforming or mapping data into UKG Dimensions."
    },
    {
      "question": "What is the general sequence of data migration steps during deployment?",
      "answer": "Foundation templates are loaded into EC, followed by position and employee master data templates via API, then data is replicated to ECP and UKG via point-to-point integration, and any remaining relevant templates are loaded into ECP and UKG via API integration."
    },
    {
      "question": "How is sensitive data protected during testing cycles?",
      "answer": "Sensitive data is identified and masked during various testing cycles, with data being scrambled and unscrambled as needed in the DataRemedy Test environment."
    },
    {
      "question": "What environments does the DataRemedy production environment integrate with?",
      "answer": "The DataRemedy production environment integrates with Development, Test, Parallel Payroll Run (PPR), and Production environments."
    },
    {
      "question": "What tool is used for data migration in the HRMIS Program?",
      "answer": "The DataRemedy tool is used for data migration in the HRMIS Program."
    },
    {
      "question": "How is sensitive data handled during migration depending on the environment?",
      "answer": "Sensitive data is scrambled depending on the environment type."
    },
    {
      "question": "What are the two methods for loading data into target systems using DataRemedy?",
      "answer": "Data can be loaded manually using templates or automatically via REST APIs."
    },
    {
      "question": "What is the maximum batch size for a single API call when loading data?",
      "answer": "A request batch size of 500 records is applied for a single API call."
    },
    {
      "question": "How are picklist type fields handled during API data loading?",
      "answer": "Picklist type fields are represented by specific numeric IDs, and mapping tables are configured for each environment to map picklist values with system-generated IDs."
    },
    {
      "question": "What happens if a data template cannot be loaded via API due to technical failures?",
      "answer": "Data templates are prepared in CSV format and can be loaded manually into the target systems."
    },
    {
      "question": "How does the HRMIS deployment approach affect data loading?",
      "answer": "The deployment is cohort-based and spans a long period, requiring data loads to accommodate changes such as employee movement and position changes."
    },
    {
      "question": "What is the process for handling errors during manual data loading?",
      "answer": "The migration team monitors load status logs, addresses data-related errors, and creates updated templates for reload."
    },
    {
      "question": "What are the two data quality uplift categories provided by DataRemedy?",
      "answer": "Auto remediation and manual remediation."
    },
    {
      "question": "How are fields that can be auto-remediated processed?",
      "answer": "They are run through auto-remediation pipelines in Talend based on defined Data Quality rules."
    },
    {
      "question": "What tool is used for manual remediation of data fields not suitable for auto remediation?",
      "answer": "Talend Data Stewardship or data cleansing activities in the data source."
    },
    {
      "question": "How many data quality issues had an approved Data Quality Uplift approach at the time of reporting?",
      "answer": "29 data quality issues had an approved Data Quality Uplift approach."
    },
    {
      "question": "What is the main goal of the position 1:1 transformation during data migration?",
      "answer": "To ensure each position contains only one employee in the HRMIS system."
    },
    {
      "question": "Which data sources are required for the position 1:1 transformation?",
      "answer": "HRISDW HRM_EMP_EMPLOYEE_DETAILS, HRISDW HRM_POSITIONS, and HRISDW HRM_EMP_POSITIONS."
    },
    {
      "question": "What rule is applied if the authorised FTE for an existing position is greater than the number of occupants?",
      "answer": "New vacant positions are created accordingly."
    },
    {
      "question": "What is the maximum authorised FTE value allowed for a new position after transformation?",
      "answer": "No position should have an authorised FTE value greater than 1."
    },
    {
      "question": "What is required for the successful implementation of the position 1:1 transformation rules?",
      "answer": "Data cleansing at the source."
    },
    {
      "question": "Why is data scrambling included in the data migration plan?",
      "answer": "To protect sensitive data when migrating it into specific environments."
    },
    {
      "question": "What is the purpose of DataRemedy in the HRMIS data migration process?",
      "answer": "DataRemedy is used to uplift data quality and migrate data to the new HRMIS solution."
    },
    {
      "question": "Who has access to unscrambled data during the data migration process?",
      "answer": "Deloitte Data Migration Developers and the HSS Data cleansing team have access to unscrambled data."
    },
    {
      "question": "How is sensitive data protected during the migration process?",
      "answer": "Sensitive data is protected by limiting system access, scrambling sensitive information where required, and following specific security protocols."
    },
    {
      "question": "What security protocols are used for data in transit and at rest in DataRemedy?",
      "answer": "Data in transit is protected using Transport Layer Security (TLS)/Single Socket Layer (SSL), and data at rest is protected using transparent data encryption."
    },
    {
      "question": "Which teams need access to both source and loading folders in Azure Blob Storage?",
      "answer": "The Data Migration Developers team needs access to both source and loading folders."
    },
    {
      "question": "What is the requirement for scrambling sensitive data during migration?",
      "answer": "Sensitive data must be scrambled into certain target environments and during different migration events to ensure it is protected or inaccessible to unauthorized individuals."
    },
    {
      "question": "What is classified as sensitive data in the HRMIS migration?",
      "answer": "Sensitive data is anything that is protected or can identify a person, such as personal details and payroll information."
    },
    {
      "question": "How is access to the DataRemedy Test and Production environments controlled?",
      "answer": "The Test environment is only accessible to Data Migration Developers, while the Production environment is accessible to multiple teams."
    },
    {
      "question": "What tool is used for data migration in the HRMIS Program?",
      "answer": "The DataRemedy tool is used for data migration in the HRMIS Program."
    },
    {
      "question": "What are the three scrambling rules applied to data during migration?",
      "answer": "The three scrambling rules are: Base data (keeping only baseline information), Delete data (deleting all the data), and Default value (using a single predefined value)."
    },
    {
      "question": "What is the purpose of the mapping metadata table in the data scrambling process?",
      "answer": "The mapping metadata table contains default values and procedures that drive the scrambling of the data."
    },
    {
      "question": "What are the two gates in the DataRemedy migration process?",
      "answer": "The two gates are Gate Ingestion and Gate Loading."
    },
    {
      "question": "What is the main purpose of data migration readiness?",
      "answer": "The main purpose of data migration readiness is to check how ready the data is to migrate into the target system by assessing data quality and verifying data after transformations."
    },
    {
      "question": "What are the two parts of the data migration readiness process?",
      "answer": "The first part focuses on the level of data quality, and the second part compares the data between the staging and mapping layer after remediation and transformation."
    },
    {
      "question": "What does the Data Migration Monitoring KPI track?",
      "answer": "The Data Migration Monitoring KPI tracks the status of the migration, including progress and any errors during the data loading process."
    },
    {
      "question": "Where is information collected for KPIs stored and visualized?",
      "answer": "Information collected for KPIs is stored within DataRemedy and visualized in PowerBI dashboards."
    },
    {
      "question": "What does the DataRemedy Reconciliation Score represent?",
      "answer": "The DataRemedy Reconciliation Score is an aggregated score that represents a comparison of key business entities when brought into DataRemedy and mapping."
    },
    {
      "question": "What are the four key areas into which migration measures are split?",
      "answer": "Migration measures are split into migration plan, progress, loading errors, and tracking."
    },
    {
      "question": "What is the purpose of displaying migration plan information in the dashboard?",
      "answer": "Displaying migration plan information allows tracking against timeframes and provides details of the plan, including environments, scrambling requirements, and load types."
    },
    {
      "question": "How are KPIs for progress and loading errors developed?",
      "answer": "KPIs for progress and loading errors are developed using information collected from the data migration process through error handling."
    },
    {
      "question": "What does the 'Open Loading Errors' KPI track?",
      "answer": "The 'Open Loading Errors' KPI tracks the number of open errors present during each migration."
    },
    {
      "question": "What is the difference between incremental load and full load in data migration?",
      "answer": "Incremental load only loads new or updated records, while full load overwrites the entire dataset with the newly updated dataset."
    },
    {
      "question": "What does ETL stand for and what are its steps?",
      "answer": "ETL stands for Extract, Transform, and Load, which are the steps of extracting data, transforming it to meet business requirements, and loading it to a target destination."
    },
    {
      "question": "What is the role of the Establishment Data Reference Group (EDRG) in the HRMIS program?",
      "answer": "The EDRG is responsible for creating a standardised Establishment Data and Workforce Framework and providing detailed SME knowledge to support Data Migration."
    },
    {
      "question": "What does the 'Progress Load completion' KPI measure?",
      "answer": "The 'Progress Load completion' KPI measures the percentage of total attempted loads that were successful."
    },
    {
      "question": "What is the main objective of the HRMIS Program described in the document?",
      "answer": "The main objective is to replace legacy HR, payroll, and rostering systems with a unified, modern platform for WA Health."
    },
    {
      "question": "Which tool is used for data migration in the HRMIS Program?",
      "answer": "The DataRemedy tool is used for data migration."
    },
    {
      "question": "How is sensitive personal data such as email addresses and phone numbers handled during data migration?",
      "answer": "Sensitive personal data is replaced with default values, such as a generic email address or phone number."
    },
    {
      "question": "What approach is taken for non-sensitive data during data migration?",
      "answer": "Non-sensitive data is left unscrambled and migrated as is."
    },
    {
      "question": "Who leads the phased implementation approach for the HRMIS Program?",
      "answer": "Health Support Services leads the phased implementation in partnership with Deloitte."
    },
    {
      "question": "What happens to sensitive data like emergency contact information and work permit document numbers?",
      "answer": "Sensitive data such as emergency contact information and work permit document numbers is deleted during migration."
    },
    {
      "question": "What is the purpose of the Data Migration Solution Design for the HRMIS Program?",
      "answer": "The purpose is to replace legacy HR, payroll, and rostering systems with a unified, modern platform for WA Health."
    },
    {
      "question": "Which tool is used for data migration in the HRMIS Program?",
      "answer": "The DataRemedy tool is used for data migration."
    },
    {
      "question": "What are some key concepts included in the Data Migration Solution Design?",
      "answer": "Key concepts include data quality uplift, security and scrambling, validation, and a phased implementation approach."
    },
    {
      "question": "Who leads the implementation of the Data Migration Solution?",
      "answer": "Health Support Services leads the implementation in partnership with Deloitte."
    },
    {
      "question": "What is the approach to software customisation in the HRMIS Program?",
      "answer": "The solution leverages commercial off-the-shelf software with minimal customisation, focusing on configuration and process improvement."
    },
    {
      "question": "What is the primary access point for job and action binaries in the Talend environment?",
      "answer": "The primary access point is https://repo.au.cloud.talend.com."
    },
    {
      "question": "Which port is used for downloading additional libraries for Talend?",
      "answer": "Port 443 (out) is used for downloading additional libraries for Talend."
    },
    {
      "question": "What is the standard port for the Talend Data Dictionary service?",
      "answer": "The standard port for the Talend Data Dictionary service is 8187 (in)."
    }
  ]
}