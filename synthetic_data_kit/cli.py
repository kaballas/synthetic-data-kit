# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the terms described in the LICENSE file in
# the root directory of this source tree.
# CLI Logic for synthetic-data-kit

import json
import os
from pathlib import Path
from typing import Any, Dict, List, Optional

import requests
import typer
from rich.console import Console
from rich.table import Table

from synthetic_data_kit.utils.config import load_config, get_vllm_config, get_openai_config, get_llm_provider, get_path_config
from synthetic_data_kit.core.context import AppContext
from synthetic_data_kit.server.app import run_server

# Initialize Typer app
app = typer.Typer(
    name="synthetic-data-kit",
    help="A toolkit for preparing synthetic datasets for fine-tuning LLMs",
    add_completion=True,
)
console = Console()

# Create app context
ctx = AppContext()


def _extract_qa_pairs_from_content(data: Any) -> List[Dict[str, Any]]:
    """Normalize different stored formats into QA pairs."""
    if isinstance(data, dict):
        for key in ("qa_pairs", "filtered_pairs"):
            value = data.get(key)
            if isinstance(value, list):
                pairs = [item for item in value if isinstance(item, dict) and "question" in item and "answer" in item]
                if pairs:
                    return pairs
        conversations = data.get("conversations")
        if isinstance(conversations, list):
            pairs = []
            for conversation in conversations:
                if not isinstance(conversation, list):
                    continue
                user_content = None
                assistant_content = None
                for message in conversation:
                    if not isinstance(message, dict):
                        continue
                    role = message.get("role")
                    if role == "user" and user_content is None:
                        user_content = message.get("content")
                    elif role == "assistant" and assistant_content is None:
                        assistant_content = message.get("content")
                if user_content and assistant_content:
                    pairs.append({"question": user_content, "answer": assistant_content})
            if pairs:
                return pairs
    elif isinstance(data, list):
        pairs = [item for item in data if isinstance(item, dict) and "question" in item and "answer" in item]
        if pairs:
            return pairs
    return []


def _load_qa_pairs_from_file(file_path: str) -> List[Dict[str, Any]]:
    """Load QA pairs from a curated artefact or raw list file."""
    path_obj = Path(file_path)
    suffix = path_obj.suffix.lower()

    if suffix == '.jsonl':
        pairs: List[Dict[str, Any]] = []
        with open(file_path, 'r', encoding='utf-8') as handle:
            for line in handle:
                record = line.strip()
                if not record:
                    continue
                item = json.loads(record)
                if isinstance(item, dict) and 'question' in item and 'answer' in item:
                    pairs.append(item)
                else:
                    extracted = _extract_qa_pairs_from_content(item)
                    if extracted:
                        pairs.extend(extracted)
        return pairs

    with open(file_path, 'r', encoding='utf-8') as handle:
        data = json.load(handle)
    return _extract_qa_pairs_from_content(data)

# Define global options
@app.callback()
def callback(
    config: Optional[Path] = typer.Option(
        None, "--config", "-c", help="Path to configuration file"
    ),
):
    """
    Global options for the Synthetic Data Kit CLI
    """
    if config:
        ctx.config_path = config
    ctx.config = load_config(ctx.config_path)


@app.command("system-check")
def system_check(
    api_base: Optional[str] = typer.Option(
        None, "--api-base", help="API base URL to check"
    ),
    provider: Optional[str] = typer.Option(
        None, "--provider", help="Provider to check ('vllm' or 'api-endpoint')"
    )
):
    """
    Check if the selected LLM provider's server is running.
    """
    # Check for API_ENDPOINT_KEY directly from environment
    console.print("Environment variable check:", style="bold blue")
    llama_key = os.environ.get('API_ENDPOINT_KEY')
    console.print(f"API_ENDPOINT_KEY: {'Present' if llama_key else 'Not found'}")
    # Debugging sanity test:
    # if llama_key:
        # console.print(f"  Value starts with: {llama_key[:10]}...")
    
    # To check the rename bug:
    #console.print("Available environment variables:", style="bold blue")
    #env_vars = [key for key in os.environ.keys() if 'API' in key or 'KEY' in key or 'TOKEN' in key]
    #for var in env_vars:
    #    console.print(f"  {var}")
    #console.print("")
    # Get provider from args or config
    selected_provider = provider or get_llm_provider(ctx.config)
    
    if selected_provider == "api-endpoint":
        # Get API endpoint config
        api_endpoint_config = get_openai_config(ctx.config)
        api_base = api_base or api_endpoint_config.get("api_base")
        
        # Check for environment variables
        api_endpoint_key = os.environ.get('API_ENDPOINT_KEY')
        console.print(f"API_ENDPOINT_KEY environment variable: {'Found' if api_endpoint_key else 'Not found'}")
        
        # Set API key with priority: env var > config
        api_key = api_endpoint_key or api_endpoint_config.get("api_key")
        if api_key:
            console.print(f"API key source: {'Environment variable' if api_endpoint_key else 'Config file'}")
        
        model = api_endpoint_config.get("model")
        
        # Check API endpoint access
        with console.status(f"Checking API endpoint access..."):
            try:
                # Try to import OpenAI
                try:
                    from openai import OpenAI
                except ImportError:
                    console.print("L API endpoint package not installed", style="red")
                    console.print("Install with: pip install openai>=1.0.0", style="yellow")
                    return 1
                
                # Create client
                client_kwargs = {}
                if api_key:
                    client_kwargs['api_key'] = api_key
                if api_base:
                    client_kwargs['base_url'] = api_base
                
                # Check API access
                try:
                    client = OpenAI(**client_kwargs)
                    # Try a simple models request to check connectivity
                    messages = [
                        {"role": "user", "content": "Hello"}
                    ]
                    response = client.chat.completions.create(
                        model=model,
                        messages=messages, 
                        temperature=0.1
                    )
                    console.print(f" API endpoint access confirmed", style="green")
                    if api_base:
                        console.print(f"Using custom API base: {api_base}", style="green")
                    console.print(f"Default model: {model}", style="green")
                    console.print(f"Response from model: {response.choices[0].message.content}", style="green")
                    return 0
                except Exception as e:
                    console.print(f"L Error connecting to API endpoint: {str(e)}", style="red")
                    if api_base:
                        console.print(f"Using custom API base: {api_base}", style="yellow")
                    if not api_key and not api_base:
                        console.print("API key is required. Set in config.yaml or as API_ENDPOINT_KEY env var", style="yellow")
                    return 1
            except Exception as e:
                console.print(f"L Error: {str(e)}", style="red")
                return 1
    else:
        # Default to vLLM
        # Get vLLM server details
        vllm_config = get_vllm_config(ctx.config)
        api_base = api_base or vllm_config.get("api_base")
        model = vllm_config.get("model")
        port = vllm_config.get("port", 8000)
        
        with console.status(f"Checking vLLM server at {api_base}..."):
            try:
                response = requests.get(f"{api_base}/models", timeout=2)
                if response.status_code == 200:
                    console.print(f" vLLM server is running at {api_base}", style="green")
                    console.print(f"Available models: {response.json()}")
                    return 0
                else:
                    console.print(f"L vLLM server is not available at {api_base}", style="red")
                    console.print(f"Error: Server returned status code: {response.status_code}")
            except requests.exceptions.RequestException as e:
                console.print(f"L vLLM server is not available at {api_base}", style="red")
                console.print(f"Error: {str(e)}")
                
            # Show instruction to start the server
            console.print("\nTo start the server, run:", style="yellow")
            console.print(f"vllm serve {model} --port {port}", style="bold blue")
            return 1


@app.command()
def ingest(
    input: str = typer.Argument(..., help="File, URL, or directory to parse"),
    output_dir: Optional[Path] = typer.Option(
        None, "--output-dir", "-o", help="Where to save the output"
    ),
    name: Optional[str] = typer.Option(
        None, "--name", "-n", help="Custom output filename (only for single files)"
    ),
    verbose: bool = typer.Option(
        False, "--verbose", "-v", help="Show detailed progress (for directories)"
    ),
    preview: bool = typer.Option(
        False, "--preview", help="Preview files to be processed without actually processing them"
    ),
    multimodal: bool = typer.Option(
        False, "--multimodal", help="Enable multimodal parsing for supported file types"
    ),
):
    """
    Parse documents (PDF, HTML, YouTube, DOCX, PPT, TXT) into clean text.
    
    Can process:
    - Single file: synthetic-data-kit ingest document.pdf
    - Directory: synthetic-data-kit ingest ./documents/
    - URL: synthetic-data-kit ingest https://example.com/page.html
    """
    import os
    from synthetic_data_kit.core.ingest import process_file
    from synthetic_data_kit.utils.directory_processor import is_directory, process_directory_ingest
    
    # Get output directory from args, then config, then default
    if output_dir is None:
        output_dir = get_path_config(ctx.config, "output", "parsed")
    
    try:
        # Check if input is a directory
        if is_directory(input):
            # Process directory
            if name is not None:
                console.print("Warning: --name option is ignored when processing directories", style="yellow")
            
            # Preview mode - show files without processing
            if preview:
                from synthetic_data_kit.utils.directory_processor import get_directory_stats, INGEST_EXTENSIONS
                
                console.print(f"Preview: scanning directory [bold]{input}[/bold]", style="blue")
                stats = get_directory_stats(input, INGEST_EXTENSIONS)
                
                if "error" in stats:
                    console.print(f"‚ùå {stats['error']}", style="red")
                    return 1
                
                console.print(f"\nüìÅ Directory: {input}")
                console.print(f"üìÑ Total files: {stats['total_files']}")
                console.print(f"‚úÖ Supported files: {stats['supported_files']}")
                console.print(f"‚ùå Unsupported files: {stats['unsupported_files']}")
                
                if stats['supported_files'] > 0:
                    console.print(f"\nüìã Files that would be processed:")
                    for ext, count in stats['by_extension'].items():
                        console.print(f"  {ext}: {count} file(s)")
                    
                    console.print(f"\nüìù File list:")
                    for filename in stats['file_list']:
                        console.print(f"  ‚Ä¢ {filename}")
                    
                    console.print(f"\nüí° To process these files, run:")
                    console.print(f"   synthetic-data-kit ingest {input} --output-dir {output_dir}", style="bold blue")
                else:
                    console.print(f"\n‚ö†Ô∏è  No supported files found.", style="yellow")
                    console.print(f"   Supported extensions: {', '.join(INGEST_EXTENSIONS)}", style="yellow")
                
                return 0
            
            console.print(f"Processing directory: [bold]{input}[/bold]", style="blue")
            results = process_directory_ingest(
                directory=input,
                output_dir=output_dir,
                config=ctx.config,
                verbose=verbose,
                multimodal=multimodal,
            )
            
            # Return appropriate exit code
            if results["failed"] > 0:
                console.print(f"‚ö†Ô∏è  Completed with {results['failed']} errors", style="yellow")
                return 1
            else:
                console.print("‚úÖ All files processed successfully!", style="green")
                return 0
        else:
            # Process single file (existing logic)
            if preview:
                console.print("Preview mode is only available for directories. Processing single file...", style="yellow")
            
            with console.status(f"Processing {input}..."):
                output_path = process_file(
                    input,
                    output_dir=output_dir,
                    output_name=name,
                    config=ctx.config,
                    multimodal=multimodal,
                )
            console.print(f"‚úÖ Text successfully extracted to [bold]{output_path}[/bold]", style="green")
            return 0
            
    except Exception as e:
        console.print(f"‚ùå Error: {e}", style="red")
        return 1


@app.command()
def create(
    input: str = typer.Argument(..., help="File or directory to process"),
    content_type: str = typer.Option(
        "qa", "--type", help="Type of content to generate [qa|summary|cot|cot-enhance|knowledge|multimodal-qa|blog]"
    ),
    output_dir: Optional[Path] = typer.Option(
        None, "--output-dir", "-o", help="Where to save the output"
    ),
    api_base: Optional[str] = typer.Option(
        None, "--api-base", help="VLLM API base URL"
    ),
    model: Optional[str] = typer.Option(
        None, "--model", "-m", help="Model to use"
    ),
    num_pairs: Optional[int] = typer.Option(
        None, "--num-pairs", "-n", help="Target number of QA pairs or CoT examples to generate"
    ),
    chunk_size: Optional[int] = typer.Option(
        None, "--chunk-size", help="Size of text chunks for processing large documents (default: 4000)"
    ),
    chunk_overlap: Optional[int] = typer.Option(
        None, "--chunk-overlap", help="Overlap between chunks in characters (default: 200)"
    ),
    verbose: bool = typer.Option(
        False, "--verbose", "-v", help="Show detailed output"
    ),
    preview: bool = typer.Option(
        False, "--preview", help="Preview files to be processed without actually processing them"
    ),
):
    """
    Generate content from text using local LLM inference.
    
    Can process:
    - Single file: synthetic-data-kit create document.txt --type qa
    - Directory: synthetic-data-kit create ./processed-text/ --type qa
    
    Content types:
    - qa: Generate question-answer pairs from .txt files (use --num-pairs to specify how many)
    - summary: Generate summaries from .txt files
    - cot: Generate Chain of Thought reasoning examples from .txt files (use --num-pairs to specify how many)
    - multimodal-qa: Generate question-answer pairs from .lance files (use --num-pairs to specify how many)
    - cot-enhance: Enhance existing conversations with Chain of Thought reasoning from .json files
      (use --num-pairs to limit the number of conversations to enhance, default is to enhance all)
      (for cot-enhance, the input must be a JSON file with either:
       - A single conversation in 'conversations' field
       - An array of conversation objects, each with a 'conversations' field
       - A direct array of conversation messages)
    - knowledge: Extract a knowledge graph from text using LLM to identify entities and relationships (from .txt files)
    - blog: Generate a blog post from .txt files
    """
    import os
    from synthetic_data_kit.core.create import process_file
    from synthetic_data_kit.utils.directory_processor import is_directory, process_directory_create, get_directory_stats, CREATE_EXTENSIONS
    
    # Check the LLM provider from config
    provider = get_llm_provider(ctx.config)
    console.print(f"üîó Using {provider} provider", style="green")
    
    if provider == "api-endpoint":
        # Use API endpoint config
        api_endpoint_config = get_openai_config(ctx.config)
        api_base = api_base or api_endpoint_config.get("api_base")
        model = model or api_endpoint_config.get("model")
        # No server check needed for API endpoint
    else:
        # Use vLLM config
        vllm_config = get_vllm_config(ctx.config)
        api_base = api_base or vllm_config.get("api_base")
        model = model or vllm_config.get("model")
        
        # Check vLLM server availability
        try:
            response = requests.get(f"{api_base}/models", timeout=2)
            if response.status_code != 200:
                console.print(f"‚ùå Error: VLLM server not available at {api_base}", style="red")
                console.print("Please start the VLLM server with:", style="yellow")
                console.print(f"vllm serve {model}", style="bold blue")
                return 1
        except requests.exceptions.RequestException:
            console.print(f"‚ùå Error: VLLM server not available at {api_base}", style="red")
            console.print("Please start the VLLM server with:", style="yellow")
            console.print(f"vllm serve {model}", style="bold blue")
            return 1
    
    # Get output directory from args, then config, then default
    if output_dir is None:
        output_dir = get_path_config(ctx.config, "output", "generated")
    
    try:
        # Check if input is a directory
        if is_directory(input) and not input.endswith(".lance"):
            # Preview mode - show files without processing
            if preview:
                # For cot-enhance, look for .json files, otherwise .txt files
                if content_type == "cot-enhance":
                    extensions = ['.json']
                elif content_type == "blog":
                    extensions = ['.txt']
                else:
                    extensions = CREATE_EXTENSIONS
                
                console.print(f"Preview: scanning directory [bold]{input}[/bold] for {content_type} processing", style="blue")
                stats = get_directory_stats(input, extensions)
                
                if "error" in stats:
                    console.print(f"‚ùå {stats['error']}", style="red")
                    return 1
                
                console.print(f"\nüìÅ Directory: {input}")
                console.print(f"üìÑ Total files: {stats['total_files']}")
                console.print(f"‚úÖ Supported files: {stats['supported_files']}")
                console.print(f"‚ùå Unsupported files: {stats['unsupported_files']}")
                
                if stats['supported_files'] > 0:
                    console.print(f"\nüìã Files that would be processed for {content_type}:")
                    for ext, count in stats['by_extension'].items():
                        console.print(f"  {ext}: {count} file(s)")
                    
                    console.print(f"\nüìù File list:")
                    for filename in stats['file_list']:
                        console.print(f"  ‚Ä¢ {filename}")
                    
                    console.print(f"\nüí° To process these files, run:")
                    console.print(f"   synthetic-data-kit create {input} --type {content_type} --output-dir {output_dir}", style="bold blue")
                else:
                    console.print(f"\n‚ö†Ô∏è  No supported files found for {content_type}.", style="yellow")
                    if content_type == "cot-enhance":
                        console.print(f"   Looking for: .json files", style="yellow")
                    else:
                        console.print(f"   Looking for: .txt files", style="yellow")
                
                return 0
            
            console.print(f"Processing directory: [bold]{input}[/bold] for {content_type} generation", style="blue")
            results = process_directory_create(
                directory=input,
                output_dir=output_dir,
                config_path=ctx.config_path,
                api_base=api_base,
                model=model,
                content_type=content_type,
                num_pairs=num_pairs,
                verbose=verbose,
                provider=provider,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
            
            # Return appropriate exit code
            if results["failed"] > 0:
                console.print(f"‚ö†Ô∏è  Completed with {results['failed']} errors", style="yellow")
                return 1
            else:
                console.print("‚úÖ All files processed successfully!", style="green")
                return 0
        else:
            # Process single file (existing logic)
            if preview:
                console.print("Preview mode is only available for directories. Processing single file...", style="yellow")
            
            with console.status(f"Generating {content_type} content from {input}..."):
                output_path = process_file(
                    input,
                    output_dir,
                    ctx.config_path,
                    api_base,
                    model,
                    content_type,
                    num_pairs,
                    verbose,
                    provider=provider,
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap
                )
            if output_path:
                console.print(f"‚úÖ Content saved to [bold]{output_path}[/bold]", style="green")
            return 0
            
    except Exception as e:
        console.print(f"‚ùå Error: {e}", style="red")
        if verbose:
            import traceback
            console.print("\n[red]Full traceback:[/red]")
            traceback.print_exc()
        return 1


@app.command("curate")
def curate(
    input: str = typer.Argument(..., help="Input file or directory to clean"),
    output: Optional[Path] = typer.Option(
        None, "--output", "-o", help="Output file path (for single files) or directory (for directories)"
    ),
    threshold: Optional[float] = typer.Option(
        None, "--threshold", "-t", help="Quality threshold (1-10)"
    ),
    api_base: Optional[str] = typer.Option(
        None, "--api-base", help="VLLM API base URL"
    ),
    model: Optional[str] = typer.Option(
        None, "--model", "-m", help="Model to use"
    ),
    verbose: bool = typer.Option(
        False, "--verbose", "-v", help="Show detailed output"
    ),
    preview: bool = typer.Option(
        False, "--preview", help="Preview files to be processed without actually processing them"
    ),
):
    """
    Clean and filter content based on quality.
    
    Can process:
    - Single file: synthetic-data-kit curate qa_pairs.json --threshold 8.0
    - Directory: synthetic-data-kit curate ./generated/ --threshold 8.0
    
    Processes .json files containing QA pairs and filters them based on quality ratings.
    """
    import os
    from synthetic_data_kit.core.curate import curate_qa_pairs
    from synthetic_data_kit.utils.directory_processor import is_directory, process_directory_curate, get_directory_stats, CURATE_EXTENSIONS
    
    # Check the LLM provider from config
    provider = get_llm_provider(ctx.config)
    
    console.print(f"üîó Using {provider} provider", style="green")
    
    if provider == "api-endpoint":
        # Use API endpoint config
        api_endpoint_config = get_openai_config(ctx.config)
        api_base = api_base or api_endpoint_config.get("api_base")
        model = model or api_endpoint_config.get("model")
        # No server check needed for API endpoint
    else:
        # Use vLLM config
        vllm_config = get_vllm_config(ctx.config)
        api_base = api_base or vllm_config.get("api_base")
        model = model or vllm_config.get("model")
        
        # Check vLLM server availability
        try:
            response = requests.get(f"{api_base}/models", timeout=2)
            if response.status_code != 200:
                console.print(f"‚ùå Error: VLLM server not available at {api_base}", style="red")
                console.print("Please start the VLLM server with:", style="yellow")
                console.print(f"vllm serve {model}", style="bold blue")
                return 1
        except requests.exceptions.RequestException:
            console.print(f"‚ùå Error: VLLM server not available at {api_base}", style="red")
            console.print("Please start the VLLM server with:", style="yellow")
            console.print(f"vllm serve {model}", style="bold blue")
            return 1
    
    try:
        # Check if input is a directory
        if is_directory(input):
            # Preview mode - show files without processing
            if preview:
                console.print(f"Preview: scanning directory [bold]{input}[/bold] for curation", style="blue")
                stats = get_directory_stats(input, CURATE_EXTENSIONS)
                
                if "error" in stats:
                    console.print(f"‚ùå {stats['error']}", style="red")
                    return 1
                
                console.print(f"\nüìÅ Directory: {input}")
                console.print(f"üìÑ Total files: {stats['total_files']}")
                console.print(f"‚úÖ Supported files: {stats['supported_files']}")
                console.print(f"‚ùå Unsupported files: {stats['unsupported_files']}")
                
                if stats['supported_files'] > 0:
                    console.print(f"\nüìã Files that would be curated:")
                    for ext, count in stats['by_extension'].items():
                        console.print(f"  {ext}: {count} file(s)")
                    
                    console.print(f"\nüìù File list:")
                    for filename in stats['file_list']:
                        console.print(f"  ‚Ä¢ {filename}")
                    
                    default_output = get_path_config(ctx.config, "output", "curated")
                    console.print(f"\nüí° To process these files, run:")
                    console.print(f"   synthetic-data-kit curate {input} --threshold {threshold or 7.0} --output {output or default_output}", style="bold blue")
                else:
                    console.print(f"\n‚ö†Ô∏è  No supported files found for curation.", style="yellow")
                    console.print(f"   Looking for: .json files with QA pairs", style="yellow")
                
                return 0
            
            # Get default output directory if not provided
            if not output:
                output = get_path_config(ctx.config, "output", "curated")
            
            console.print(f"Processing directory: [bold]{input}[/bold] for curation", style="blue")
            results = process_directory_curate(
                directory=input,
                output_dir=output,
                threshold=threshold,
                api_base=api_base,
                model=model,
                config_path=ctx.config_path,
                verbose=verbose,
                provider=provider
            )
            
            # Return appropriate exit code
            if results["failed"] > 0:
                console.print(f"‚ö†Ô∏è  Completed with {results['failed']} errors", style="yellow")
                return 1
            else:
                console.print("‚úÖ All files processed successfully!", style="green")
                return 0
        else:
            # Process single file (existing logic)
            if preview:
                console.print("Preview mode is only available for directories. Processing single file...", style="yellow")
            
            # Get default output path from config if not provided
            if not output:
                curated_dir = get_path_config(ctx.config, "output", "curated")
                os.makedirs(curated_dir, exist_ok=True)
                base_name = os.path.splitext(os.path.basename(input))[0]
                output = os.path.join(curated_dir, f"{base_name}_cleaned.json")
            
            with console.status(f"Cleaning content from {input}..."):
                result_path = curate_qa_pairs(
                    input,
                    output,
                    threshold,
                    api_base,
                    model,
                    ctx.config_path,
                    verbose,
                    provider=provider
                )
            console.print(f"‚úÖ Cleaned content saved to [bold]{result_path}[/bold]", style="green")
            return 0
            
    except Exception as e:
        console.print(f"‚ùå Error: {e}", style="red")
        return 1


@app.command("save-as")
def save_as(
    input: str = typer.Argument(..., help="Input file or directory to convert"),
    format: Optional[str] = typer.Option(
        None, "--format", "-f", help="Output format [jsonl|alpaca|ft|chatml]"
    ),
    storage: str = typer.Option(
        "json", "--storage", help="Storage format [json|hf]",
        show_default=True
    ),
    output: Optional[Path] = typer.Option(
        None, "--output", "-o", help="Output file path (for single files) or directory (for directories)"
    ),
    verbose: bool = typer.Option(
        False, "--verbose", "-v", help="Show detailed output"
    ),
    preview: bool = typer.Option(
        False, "--preview", help="Preview files to be processed without actually processing them"
    ),
):
    """
    Convert to different formats for fine-tuning.
    
    Can process:
    - Single file: synthetic-data-kit save-as curated_file.json --format alpaca
    - Directory: synthetic-data-kit save-as ./curated/ --format alpaca
    
    The --format option controls the content format (how the data is structured).
    The --storage option controls how the data is stored (JSON file or HF dataset).
    
    When using --storage hf, the output will be a directory containing a Hugging Face 
    dataset in Arrow format, which is optimized for machine learning workflows.
    
    Processes .json files containing curated QA pairs and converts them to training formats.
    """
    import os
    from synthetic_data_kit.core.save_as import convert_format
    from synthetic_data_kit.utils.directory_processor import is_directory, process_directory_save_as, get_directory_stats, SAVE_AS_EXTENSIONS
    
    # Get format from args or config
    if not format:
        format_config = ctx.config.get("format", {})
        format = format_config.get("default", "jsonl")
    
    try:
        # Check if input is a directory
        if is_directory(input):
            # Preview mode - show files without processing
            if preview:
                console.print(f"Preview: scanning directory [bold]{input}[/bold] for format conversion", style="blue")
                stats = get_directory_stats(input, SAVE_AS_EXTENSIONS)
                
                if "error" in stats:
                    console.print(f"‚ùå {stats['error']}", style="red")
                    return 1
                
                console.print(f"\nüìÅ Directory: {input}")
                console.print(f"üìÑ Total files: {stats['total_files']}")
                console.print(f"‚úÖ Supported files: {stats['supported_files']}")
                console.print(f"‚ùå Unsupported files: {stats['unsupported_files']}")
                
                if stats['supported_files'] > 0:
                    console.print(f"\nüìã Files that would be converted to {format} format:")
                    for ext, count in stats['by_extension'].items():
                        console.print(f"  {ext}: {count} file(s)")
                    
                    console.print(f"\nüìù File list:")
                    for filename in stats['file_list']:
                        console.print(f"  ‚Ä¢ {filename}")
                    
                    default_output = get_path_config(ctx.config, "output", "final")
                    console.print(f"\nüí° To process these files, run:")
                    console.print(f"   synthetic-data-kit save-as {input} --format {format} --storage {storage} --output {output or default_output}", style="bold blue")
                else:
                    console.print(f"\n‚ö†Ô∏è  No supported files found for format conversion.", style="yellow")
                    console.print(f"   Looking for: .json files with curated QA pairs", style="yellow")
                
                return 0
            
            # Get default output directory if not provided
            if not output:
                output = get_path_config(ctx.config, "output", "final")
            
            console.print(f"Processing directory: [bold]{input}[/bold] for format conversion to {format}", style="blue")
            results = process_directory_save_as(
                directory=input,
                output_dir=output,
                format=format,
                storage_format=storage,
                config=ctx.config,
                verbose=verbose
            )
            
            # Return appropriate exit code
            if results["failed"] > 0:
                console.print(f"‚ö†Ô∏è  Completed with {results['failed']} errors", style="yellow")
                return 1
            else:
                console.print("‚úÖ All files converted successfully!", style="green")
                return 0
        else:
            # Process single file (existing logic)
            if preview:
                console.print("Preview mode is only available for directories. Processing single file...", style="yellow")
            
            # Set default output path if not provided
            if not output:
                final_dir = get_path_config(ctx.config, "output", "final")
                os.makedirs(final_dir, exist_ok=True)
                base_name = os.path.splitext(os.path.basename(input))[0]
                
                if storage == "hf":
                    # For HF datasets, use a directory name
                    output = os.path.join(final_dir, f"{base_name}_{format}_hf")
                else:
                    # For JSON files, use appropriate extension
                    if format == "jsonl":
                        output = os.path.join(final_dir, f"{base_name}.jsonl")
                    else:
                        output = os.path.join(final_dir, f"{base_name}_{format}.json")
            
            with console.status(f"Converting {input} to {format} format with {storage} storage..."):
                output_path = convert_format(
                    input,
                    output,
                    format,
                    ctx.config,
                    storage_format=storage
                )
            
            if storage == "hf":
                console.print(f"‚úÖ Converted to {format} format and saved as HF dataset to [bold]{output_path}[/bold]", style="green")
            else:
                console.print(f"‚úÖ Converted to {format} format and saved to [bold]{output_path}[/bold]", style="green")
            return 0
            
    except Exception as e:
        console.print(f"‚ùå Error: {e}", style="red")
        return 1


@app.command("generate-knowledge-graph")
def generate_knowledge_graph_cli(
    input: str = typer.Argument(..., help="File or directory of curated QA pairs"),
    output: Optional[Path] = typer.Option(
        None, "--output", "-o", help="Path to save the generated knowledge graph JSON"
    ),
    max_nodes: int = typer.Option(
        50, "--max-nodes", help="Maximum number of nodes to include"
    ),
    max_edges: int = typer.Option(
        200, "--max-edges", help="Maximum number of edges to include"
    ),
    min_cooccurrence: float = typer.Option(
        1.0, "--min-cooccurrence", help="Minimum co-occurrence weight for edges"
    ),
    # Older Typer/Click versions may not support `multiple=True` on Option.
    # Accept a single string (comma-separated) or repeated flags depending on the
    # installed Typer behavior. We'll parse the value(s) into a list below.
    stopwords: Optional[str] = typer.Option(
        None,
        "--stopword",
        "-s",
        help="Additional stopwords to exclude (comma-separated or repeat flag for multiple values)",
    ),
    preview: bool = typer.Option(
        False, "--preview", help="Show summary without writing the graph"
    ),
    verbose: bool = typer.Option(
        False, "--verbose", "-v", help="Show detailed loading information"
    ),
):
    """Generate a lightweight knowledge graph from curated QA pairs."""
    from synthetic_data_kit.utils.directory_processor import get_supported_files, is_directory
    from synthetic_data_kit.utils.knowledge_graph import generate_knowledge_graph as build_graph

    graph_extensions = ['.json', '.jsonl']
    input_path = Path(input)
    # Normalize stopwords into a list. Support three cases:
    # - None/empty -> None
    # - A single comma-separated string -> split into list
    # - A list (if Typer/Click returned multiple flags as a list) -> flatten and split
    stopword_list = []
    if stopwords:
        # If a list was passed (some Typer versions may return a list), handle it
        if isinstance(stopwords, list):
            for item in stopwords:
                if item:
                    stopword_list.extend([w.strip() for w in str(item).split(",") if w.strip()])
        else:
            # Single string: allow comma-separated values
            stopword_list = [w.strip() for w in str(stopwords).split(",") if w.strip()]

    if not stopword_list:
        stopword_list = None

    if is_directory(str(input_path)):
        files = get_supported_files(str(input_path), graph_extensions)
        if not files:
            console.print(f"No supported files found in {input_path}", style="yellow")
            return 1
    else:
        if not input_path.exists():
            console.print(f"Input path not found: {input_path}", style="red")
            return 1
        if input_path.suffix.lower() not in graph_extensions:
            console.print("Input file must be a .json or .jsonl artifact containing QA pairs.", style="red")
            return 1
        files = [str(input_path)]

    all_pairs: List[Dict[str, Any]] = []
    errors: List[str] = []

    for file_str in files:
        try:
            pairs = _load_qa_pairs_from_file(file_str)
        except json.JSONDecodeError as exc:
            message = f"{file_str}: {exc}"
            errors.append(message)
            if verbose:
                console.print(f"Failed to parse {file_str}: {exc}", style="red")
            continue

        if not pairs:
            if verbose:
                console.print(f"No QA pairs found in {file_str}", style="yellow")
            continue

        all_pairs.extend(pairs)
        if verbose:
            console.print(f"Loaded {len(pairs)} pair(s) from {file_str}", style="blue")

    if not all_pairs:
        console.print("No QA pairs available to build a knowledge graph.", style="red")
        if errors:
            console.print("Files skipped due to errors:", style="yellow")
            for message in errors:
                console.print(f"  - {message}", style="yellow")
        return 1

    graph = build_graph(
        all_pairs,
        max_nodes=max_nodes,
        max_edges=max_edges,
        min_cooccurrence=min_cooccurrence,
        additional_stopwords=stopword_list,
    )

    if preview:
        console.print(
            f"Preview: {len(graph['nodes'])} nodes, {len(graph['edges'])} edges from {len(all_pairs)} QA pairs.",
            style="blue",
        )
        if errors:
            console.print("Files skipped due to errors:", style="yellow")
            for message in errors:
                console.print(f"  - {message}", style="yellow")
        return 0

    if output is None:
        if is_directory(str(input_path)):
            output_path = input_path / "knowledge_graph.json"
        else:
            output_path = input_path.with_name(f"{input_path.stem}_knowledge_graph.json")
    else:
        output_path = Path(output)
        if output_path.is_dir():
            output_path = output_path / "knowledge_graph.json"

    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(json.dumps(graph, indent=2), encoding='utf-8')

    console.print(f"Knowledge graph saved to {output_path}", style="green")
    if errors:
        console.print("Files skipped due to errors:", style="yellow")
        for message in errors:
            console.print(f"  - {message}", style="yellow")
    return 0


@app.command("podcast")
def podcast(
    input: str = typer.Argument(..., help="File or directory to convert to podcast"),
    output_dir: Optional[Path] = typer.Option(
        None, "--output-dir", "-o", help="Where to save the podcast outputs"
    ),
    api_base: Optional[str] = typer.Option(
        None, "--api-base", help="LLM API base URL"
    ),
    model: Optional[str] = typer.Option(
        None, "--model", "-m", help="Model to use for dialogue generation"
    ),
    generate_audio: bool = typer.Option(
        False, "--audio", "-a", help="Generate audio file (requires TTS setup)"
    ),
    tts_provider: Optional[str] = typer.Option(
        None, "--tts", help="TTS provider [openai|elevenlabs|edge|gemini|geminimulti]"
    ),
    num_chunks: Optional[int] = typer.Option(
        None, "--num-chunks", help="Number of dialogue chunks for long content"
    ),
    verbose: bool = typer.Option(
        False, "--verbose", "-v", help="Show detailed output"
    ),
    preview: bool = typer.Option(
        False, "--preview", help="Preview files to be processed without actually processing them"
    ),
):
    """
    Convert text documents to podcast dialogues.
    
    Can process:
    - Single file: synthetic-data-kit podcast document.txt
    - Directory: synthetic-data-kit podcast ./data/parsed/
    
    Generates:
    - Transcript file (always generated)
    - Dialogue JSON (always generated)
    - Audio file (optional, with --audio flag)
    
    TTS Providers:
    - openai: OpenAI TTS (requires API key)
    - elevenlabs: ElevenLabs (requires API key)
    - edge: Edge TTS (free, no API key needed)
    - gemini: Google Gemini TTS (requires API key)
    
    Examples:
        # Generate transcript only
        synthetic-data-kit podcast document.txt
        
        # Generate transcript and audio with OpenAI
        synthetic-data-kit podcast document.txt --audio --tts openai
        
        # Process entire directory with Edge TTS (free)
        synthetic-data-kit podcast ./data/parsed/ --audio --tts edge -v
    """
    import os
    from synthetic_data_kit.utils.directory_processor import is_directory
    
    # Check the LLM provider from config
    provider = get_llm_provider(ctx.config)
    console.print(f"üîó Using {provider} provider for dialogue generation", style="green")
    
    if provider == "api-endpoint":
        # Use API endpoint config
        api_endpoint_config = get_openai_config(ctx.config)
        api_base = api_base or api_endpoint_config.get("api_base")
        model = model or api_endpoint_config.get("model")
    else:
        # Use vLLM config
        vllm_config = get_vllm_config(ctx.config)
        api_base = api_base or vllm_config.get("api_base")
        model = model or vllm_config.get("model")
        
        # Check vLLM server availability
        try:
            response = requests.get(f"{api_base}/models", timeout=2)
            if response.status_code != 200:
                console.print(f"‚ùå Error: VLLM server not available at {api_base}", style="red")
                console.print("Please start the VLLM server with:", style="yellow")
                console.print(f"vllm serve {model}", style="bold blue")
                return 1
        except requests.exceptions.RequestException:
            console.print(f"‚ùå Error: VLLM server not available at {api_base}", style="red")
            console.print("Please start the VLLM server with:", style="yellow")
            console.print(f"vllm serve {model}", style="bold blue")
            return 1
    
    # Get output directory from args, then config, then default
    if output_dir is None:
        output_dir = get_path_config(ctx.config, "output", "generated")
    
    # Get TTS provider from args or config
    if tts_provider is None:
        tts_provider = ctx.config.get("podcast", {}).get("text_to_speech", {}).get("default_tts_model", "openai")
    
    try:
        # Check if input is a directory
        if is_directory(input):
            # Preview mode - show files without processing
            if preview:
                from synthetic_data_kit.utils.directory_processor import get_directory_stats
                
                console.print(f"Preview: scanning directory [bold]{input}[/bold] for podcast generation", style="blue")
                stats = get_directory_stats(input, ['.txt'])
                
                if "error" in stats:
                    console.print(f"‚ùå {stats['error']}", style="red")
                    return 1
                
                console.print(f"\nüìÅ Directory: {input}")
                console.print(f"üìÑ Total files: {stats['total_files']}")
                console.print(f"‚úÖ Supported files: {stats['supported_files']}")
                console.print(f"‚ùå Unsupported files: {stats['unsupported_files']}")
                
                if stats['supported_files'] > 0:
                    console.print(f"\nüìã Files that would be converted to podcasts:")
                    for filename in stats['file_list']:
                        console.print(f"  ‚Ä¢ {filename}")
                    
                    console.print(f"\nüí° To process these files, run:")
                    audio_flag = "--audio --tts " + tts_provider if generate_audio else ""
                    console.print(f"   synthetic-data-kit podcast {input} {audio_flag}", style="bold blue")
                else:
                    console.print(f"\n‚ö†Ô∏è  No .txt files found for podcast generation.", style="yellow")
                
                return 0
            
            # Process directory
            from synthetic_data_kit.utils.directory_processor import process_directory_podcast
            
            console.print(f"Processing directory: [bold]{input}[/bold] for podcast generation", style="blue")
            if generate_audio:
                console.print(f"Audio generation enabled with {tts_provider} TTS", style="cyan")
            
            results = process_directory_podcast(
                directory=input,
                output_dir=output_dir,
                config_path=ctx.config_path,
                api_base=api_base,
                model=model,
                generate_audio=generate_audio,
                tts_provider=tts_provider,
                num_chunks=num_chunks,
                verbose=verbose,
                provider=provider
            )
            
            # Return appropriate exit code
            if results["failed"] > 0:
                console.print(f"‚ö†Ô∏è  Completed with {results['failed']} errors", style="yellow")
                return 1
            else:
                console.print("‚úÖ All podcasts generated successfully!", style="green")
                return 0
        else:
            # Process single file
            if preview:
                console.print("Preview mode is only available for directories. Processing single file...", style="yellow")
            
            from synthetic_data_kit.core.podcast import process_file
            
            if generate_audio:
                console.print(f"Generating podcast with audio ({tts_provider} TTS)...", style="cyan")
            
            with console.status(f"Generating podcast from {input}..."):
                result = process_file(
                    input,
                    output_dir,
                    ctx.config_path,
                    api_base,
                    model,
                    generate_audio,
                    tts_provider,
                    num_chunks,
                    verbose,
                    provider=provider
                )
            
            console.print(f"‚úÖ Podcast transcript saved to [bold]{result['transcript_path']}[/bold]", style="green")
            if "audio_path" in result:
                console.print(f"üîä Audio saved to [bold]{result['audio_path']}[/bold]", style="green")
            
            return 0
            
    except Exception as e:
        console.print(f"‚ùå Error: {e}", style="red")
        if verbose:
            import traceback
            console.print("\n[red]Full traceback:[/red]")
            traceback.print_exc()
        return 1


@app.command("server")
def server(
    host: str = typer.Option(
        "127.0.0.1", "--host", help="Host address to bind the server to"
    ),
    port: int = typer.Option(
        5000, "--port", "-p", help="Port to run the server on"
    ),
    debug: bool = typer.Option(
        False, "--debug", "-d", help="Run the server in debug mode"
    ),
):
    """
    Start a web interface for the Synthetic Data Kit.
    
    This launches a web server that provides a UI for all SDK functionality,
    including generating and curating QA pairs, as well as viewing
    and managing generated files.
    """
    provider = get_llm_provider(ctx.config)
    console.print(f"Starting web server with {provider} provider...", style="green")
    console.print(f"Web interface available at: http://{host}:{port}", style="bold green")
    console.print("Press CTRL+C to stop the server.", style="italic")
    
    # Run the Flask server
    run_server(host=host, port=port, debug=debug)


if __name__ == "__main__":
    app()