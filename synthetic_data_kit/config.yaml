# Master configuration file for Synthetic Data Kit

# Global paths configuration
paths:
  # Input data location (directory containing files to process)
  input: "data/input"           # Directory containing PDF, HTML, DOCX, PPT, TXT files
  
  # Output locations (4-stage pipeline directories)
  output:
    parsed: "data/parsed"       # Stage 1: Where parsed text files are saved (ingest output)
    generated: "data/generated" # Stage 2: Where generated QA pairs are saved (create output)
    curated: "data/curated"     # Stage 3: Where curated QA pairs are saved (curate output)
    final: "data/final"         # Stage 4: Where final training formats are saved (save-as output)

# LLM Provider configuration
llm:
  # Provider selection: "vllm" or "api-endpoint"
  provider: "api-endpoint"

# VLLM server configuration
vllm:
  api_base: "http://localhost:8000/v1" # Base URL for VLLM API
  port: 8000                           # Port for VLLM server
  model: "meta-llama/Llama-3.3-70B-Instruct" # Default model to use
  max_retries: 3                       # Number of retries for API calls
  retry_delay: 1.0                     # Initial delay between retries (seconds)
  
# API endpoint configuration
api-endpoint:
  api_base: "http://localhost:8001/v1" # Optional base URL for API endpoint (null for default API)
  api_key: "llama-api-key"               # API key for API endpoint or compatible service (can use env var instead)
  model: "gpt-5-mini" # Default model to use
  max_retries: 3                       # Number of retries for API calls
  retry_delay: 1.0                     # Initial delay between retries (seconds)

# Ingest configuration
ingest:
  default_format: "txt"  # Default output format for parsed files
  youtube_captions: "auto"  # Options: "auto", "manual" - caption preference

# LLM generation parameters
generation:
  temperature: 0.7   # Higher = more creative, lower = more deterministic
  top_p: 0.95        # Nucleus sampling parameter
  
  # Document processing strategy
  # "auto": choose based on document size, "single": force single call, "chunking": force chunking
  processing_strategy: "chunking"
  single_call_max_size: 8000  # Documents smaller than this use single call processing
  
  # Chunking parameters (used for large documents)
  chunk_size: 8000   # Size of text chunks for processing large documents
  overlap: 200       # Overlap between chunks to maintain context (prevents losing info at boundaries)
  
  # Model parameters
  max_tokens: 4096   # Maximum tokens in LLM responses
  
  # Content generation targets
  num_pairs: 1000      # Default number of QA pairs to generate
  num_cot_examples: 5  # Default number of Chain of Thought examples to generate
  num_cot_enhance_examples: null  # Maximum number of conversations to enhance (null = enhance all)
  
  # Batch processing
  batch_size: 1     # Number of requests to batch together (for create)
  
  # Quality settings
  enable_deduplication: true    # Remove very similar questions/examples
  similarity_threshold: 0.8     # Threshold for considering items similar (0.0-1.0)

# Content curation parameters
curate:
  threshold: 7.0     # Default quality threshold (1-10)
  batch_size: 1      # Number of items per batch for rating (smaller batches for API stability)
  inference_batch: 1 # Number of batches to process at once with VLLM
  temperature: 0.1   # Temperature for rating (lower = more consistent)

# Format conversion parameters
format:
  default: "jsonl"   # Default output format
  include_metadata: true  # Include metadata in output files
  pretty_json: true  # Use indentation in JSON output

# Prompts for different tasks
prompts:
  # Summary generation prompt
  summary: |
    Summarize this document in 3-5 sentences, focusing on the main topic and key concepts.
  
  # QA pair generation prompt
  qa_generation: |

    summary : {summary}
    Create question-answer pairs from this text for LLM training.
    
    Rules:
    1. Questions must be about important facts in the text
    2. Answers must be directly supported by the text
    3. Return JSON format only:
    
    [
      {{
        "question": "Question 1?",
        "answer": "Answer 1."
      }},
      {{
        "question": "Question 2?",
        "answer": "Answer 2."
      }}
    ]
    
    Text:
    {text}
  
  # QA pair rating prompt
  qa_rating: |
    Rate each question-answer pair on a scale from 1-10, based on:
    - Accuracy (0-3): factual correctness
    - Relevance (0-2): relevance to content
    - Clarity (0-2): clear language
    - Usefulness (0-3): value for model learning
    
    YOU MUST RETURN A VALID JSON OBJECT OR ARRAY WITH THIS EXACT SCHEMA:
    {{
      "question": "Exact question text",
      "answer": "Exact answer text",
      "rating": 8
    }}
    
    OR FOR MULTIPLE PAIRS:
    [
      {{"question": "Q1", "answer": "A1", "rating": 8}},
      {{"question": "Q2", "answer": "A2", "rating": 9}}
    ]
    
    *** YOUR RESPONSE MUST BE VALID JSON AND NOTHING ELSE - NO EXPLANATION, NO MARKDOWN ***
    
    QA pairs to rate:
    {pairs}
    
  # Chain of Thought generation prompt
  cot_generation_backup: |
    Create complex reasoning examples from this text that demonstrate chain-of-thought thinking.
    
    Each example should have:
    1. A challenging question that requires step-by-step reasoning
    2. Detailed reasoning steps that break down the problem
    3. A concise final answer
    
    Return JSON format only:
    
    [
      {{
        "question": "Complex question about the text?",
        "reasoning": "Step 1: First, I need to consider...\nStep 2: Then, I analyze...\nStep 3: Finally, I can conclude...",
        "answer": "Final answer based on the reasoning."
      }},
      {{
        "question": "Another complex question?",
        "reasoning": "Step 1: First, I'll analyze...\nStep 2: Next, I need to determine...\nStep 3: Based on this analysis...",
        "answer": "Final answer drawn from the reasoning."
      }}
    ]
    
    Text:
    {text}

  # Chain of Thought Generation Prompt
  cot_generation: |
    Begin with a concise checklist (3-7 bullets) describing the steps you will take to generate each reasoning example.

    IMPORTANT: The input text may contain JSON-like fragments or code snippets. Ignore these and focus only on generating reasoning examples from the actual content about the topic.

    Generate complex reasoning examples from the provided text that exhibit chain-of-thought thinking.

    For each example, include:
    1. A challenging question that necessitates step-by-step reasoning.
    2. Detailed reasoning steps that decompose the problem logically.
    3. A concise final answer derived from the reasoning process.

    Return the output strictly in JSON format using the required schema below. Do not include any text before or after the JSON.

    After generating all examples, validate that each object contains the "question", "reasoning", and "answer" fields as non-empty strings. If validation fails, self-correct or return an empty array if the input did not allow valid results.

    ## Output Format
    The response must be a valid JSON array (with at least 2 elements), each being an object with the following string fields:
    - "question": A challenging, text-relevant question requiring multi-step reasoning.
    - "reasoning": A detailed, step-by-step explanation outlining the logical approach.
    - "answer": A concise answer obtained from the provided reasoning.

    Example:
    [
      {{
        "question": "Complex question about the text?",
        "reasoning": "Step 1: First, I need to consider...\nStep 2: Then, I analyze...\nStep 3: Finally, I can conclude...",
        "answer": "Final answer based on the reasoning."
      }},
      {{
        "question": "Another complex question?",
        "reasoning": "Step 1: First, I'll analyze...\nStep 2: Next, I need to determine...\nStep 3: Based on this analysis...",
        "answer": "Final answer drawn from the reasoning."
      }}
    ]
    
    - Generate at least two examples for each input text. You may include more if relevant.
    - Keep the array ordered according to your example generation sequence; order need not reflect the input text order but should remain logical and clear.
    - All field values must be strings. While there are no strict string length limits, maintain clarity and conciseness.
    - If the provided text is empty or lacks content suitable for reasoning, return an empty JSON array: [].
    - If the input is too ambiguous to formulate reasonable questions, return an empty JSON array: []. 
    
    Return JSON format only:

    Text:
    {text}   
  
  # Chain of Thought enhancement prompt
  cot_enhancement: |
    You are an expert reasoning assistant. Your task is to enhance the given conversations by adding chain-of-thought reasoning.
    
    For each conversation, add detailed step-by-step reasoning to the assistant's responses while preserving the original answer.
    
    {include_simple_steps} = Whether to add reasoning to simple responses too. If false, only add reasoning to complex responses.
    
    Return the enhanced conversations as a JSON array matching this format:
    [
      [
        {{"role": "system", "content": "System message"}},
        {{"role": "user", "content": "User question"}},
        {{"role": "assistant", "content": "Let me think through this step by step:\n\n1. First, I need to consider...\n2. Then...\n\nTherefore, [original answer]"}}
      ],
      [
        {{"role": "system", "content": "System message"}},
        {{"role": "user", "content": "Another user question"}},
        {{"role": "assistant", "content": "Let me work through this:\n\n1. I'll start by...\n2. Next...\n\nIn conclusion, [original answer]"}}
      ]
    ]
    
    Original conversations:
    {conversations}


  knowledge: |
    - You are tasked with configuring a prompt-based system for an expert financial analyst to extract a knowledge graph from a 10-K filing, focusing on detailed and strategic company information.

    # Instructions
    - Begin with a concise checklist (3-7 bullets) of what you will do; keep items conceptual, not implementation-level.
    - Extract a comprehensive and meaningful knowledge graph, capturing critical entities and their strategic interconnections from the provided 10-K text segment.
      
    ## Nodes to Extract
    - Identify and create nodes for:
        - Company
        - Business Segments
        - Products/Services
        - Financial Metrics (e.g., Total Revenue, Net Income, Segment Revenues)
        - Legal Proceedings (significant ones only)
        - Major Risk Factors
        - Key People (executive roles)

    ## Relationships to Create
    - Construct relationships capturing how entities are strategically linked. Examples include:
        - (Company)-[HAS_SEGMENT]->(Business Segment)
        - (Business Segment)-[GENERATES]->(Financial Metric)
        - (Company)-[FACES_RISK]->(Risk Factor)
        - (Company)-[LED_BY]->(Person)
        - (Company)-[INVOLVED_IN]->(Legal Proceeding)
    - Strive for rich graph density: capture a thorough overview with approximately 30â€“60 nodes and up to 200 significant relationships.

    # Output Format
    Return a JSON object with the following schema:
    ```json
    {
      "nodes": [
        {
          "id": "string (unique)",
          "type": "string (e.g., Company, Segment, Product, Person, Risk, LegalProceeding, FinancialMetric)",
          "name": "string (canonical name)",
          "attributes": { "key1": "value1", ... } // Optional, for relevant details
        }
        // ... more nodes
      ],
      "relationships": [
        {
          "source_id": "string (node id)",
          "target_id": "string (node id)",
          "type": "string (e.g., HAS_SEGMENT, GENERATES, FACES_RISK, LED_BY, INVOLVED_IN)",
          "attributes": { "key1": "value1", ... } // Optional, for relevant details
        }
        // ... more relationships
      ],
      "errors": [
        {
          "message": "string explaining any issues encountered (e.g., missing information, ambiguous entities, unexpected input format)",
          "section": "string (where the error occurred)"
        }
        // ... more errors if applicable
      ]
    }
    ```

    - If certain expected nodes or relationships are not present or cannot be inferred, omit them from the returned lists but include a suitable description in the `errors` list.
    - Sort nodes by type (Company first, then Segment, then remaining types alphabetically) and relationships by `source_id` then `type`.
    - Use clear, unique node IDs throughout.

    # Example Human Prompt
    - Use the following human message template with input 10-K data:

        Use the given format to extract information from the following text:

        {input}

    # Reasoning and Verification
    - Parse the 10-K text step-by-step to identify entities and relationships per definitions above.
    - Internally verify that extracted nodes and relationships match expectations for coverage and accuracy.
    - Set reasoning_effort=high given the complexity and detail of the task; ensure output fully addresses the JSON schema.

    # Output Verbosity
    - Output should be complete but concise, focusing on required schema and error reporting only.

    # Post-action Validation
    - After extracting the knowledge graph, validate completeness against schema requirements and confirm all candidate nodes and relationships have been reviewed. If gaps or errors are detected, attempt corrective adjustments before final output.

    # Stop Conditions
    - Finish when all possible nodes and relationships are extracted or accounted for in the output format (including errors where information is missing or ambiguous).

